{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (2.0.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (1.5.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (3.9.2)\n",
      "Requirement already satisfied: seaborn in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (0.13.2)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (2.18.0)\n",
      "Requirement already satisfied: keras in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib) (4.55.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (5.28.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (75.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.68.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: rich in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from keras) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from keras) (0.13.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from rich->keras) (2.18.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.45.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\alibh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "!pip install numpy pandas scikit-learn matplotlib seaborn tensorflow keras\n",
    "\n",
    "# Import common ML libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 111/111 [09:25<00:00,  5.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing testing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [03:06<00:00,  9.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data processing complete!\n",
      "Data generators created and ready for model training!\n"
     ]
    }
   ],
   "source": [
    "# Set paths for data directories\n",
    "train_ct_path = r\"X:\\Cursor\\LiTS_Venv\\LiTS(train_test)\\train_CT\"\n",
    "train_mask_path = r\"X:\\Cursor\\LiTS_Venv\\LiTS(train_test)\\train_mask\"\n",
    "test_ct_path = r\"X:\\Cursor\\LiTS_Venv\\LiTS(train_test)\\test_CT\" \n",
    "test_mask_path = r\"X:\\Cursor\\LiTS_Venv\\LiTS(train_test)\\test_mask\"\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import nibabel as nib\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "\n",
    "def process_nifti_files(ct_path, mask_path, output_path, is_training=True):\n",
    "    \"\"\"\n",
    "    Process .nii files and convert them to normalized numpy arrays\n",
    "    \"\"\"\n",
    "    # Create output directories if they don't exist\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    ct_output = os.path.join(output_path, 'ct_scans')\n",
    "    mask_output = os.path.join(output_path, 'masks')\n",
    "    os.makedirs(ct_output, exist_ok=True)\n",
    "    os.makedirs(mask_output, exist_ok=True)\n",
    "\n",
    "    # Get list of files\n",
    "    ct_files = sorted([f for f in os.listdir(ct_path) if f.endswith('.nii')])\n",
    "    \n",
    "    print(f\"Processing {'training' if is_training else 'testing'} data...\")\n",
    "    \n",
    "    for ct_file in tqdm(ct_files):\n",
    "        try:\n",
    "            # Load CT scan\n",
    "            ct_nifti = nib.load(os.path.join(ct_path, ct_file))\n",
    "            ct_data = ct_nifti.get_fdata()\n",
    "            \n",
    "            # Load corresponding mask\n",
    "            mask_file = f\"segmentation-{ct_file.split('-')[1]}\"\n",
    "            mask_nifti = nib.load(os.path.join(mask_path, mask_file))\n",
    "            mask_data = mask_nifti.get_fdata()\n",
    "            \n",
    "            # Normalize CT data to 0-1 range\n",
    "            ct_data = (ct_data - ct_data.min()) / (ct_data.max() - ct_data.min())\n",
    "            \n",
    "            # Convert mask to binary (0 or 1)\n",
    "            mask_data = (mask_data > 0).astype(np.float32)\n",
    "            \n",
    "            # Save each slice as a separate file\n",
    "            for slice_idx in range(ct_data.shape[2]):\n",
    "                ct_slice = ct_data[:,:,slice_idx]\n",
    "                mask_slice = mask_data[:,:,slice_idx]\n",
    "                \n",
    "                # Resize to standard size (e.g., 256x256)\n",
    "                ct_slice = cv2.resize(ct_slice, (256, 256))\n",
    "                mask_slice = cv2.resize(mask_slice, (256, 256))\n",
    "                \n",
    "                # Save as numpy arrays\n",
    "                slice_name = f\"{ct_file.split('.')[0]}_slice_{slice_idx}\"\n",
    "                np.save(os.path.join(ct_output, f\"{slice_name}.npy\"), ct_slice)\n",
    "                np.save(os.path.join(mask_output, f\"{slice_name}.npy\"), mask_slice)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {ct_file}: {str(e)}\")\n",
    "\n",
    "# Process training data\n",
    "train_output = \"processed_data/train\"\n",
    "process_nifti_files(train_ct_path, train_mask_path, train_output, is_training=True)\n",
    "\n",
    "# Process testing data\n",
    "test_output = \"processed_data/test\"\n",
    "process_nifti_files(test_ct_path, test_mask_path, test_output, is_training=False)\n",
    "\n",
    "print(\"\\nData processing complete!\")\n",
    "\n",
    "# Create data generators for training\n",
    "def create_data_generator(data_path, batch_size=32):\n",
    "    ct_path = os.path.join(data_path, 'ct_scans')\n",
    "    mask_path = os.path.join(data_path, 'masks')\n",
    "    \n",
    "    ct_files = sorted([f for f in os.listdir(ct_path) if f.endswith('.npy')])\n",
    "    \n",
    "    while True:\n",
    "        # Shuffle files at the start of each epoch\n",
    "        np.random.shuffle(ct_files)\n",
    "        \n",
    "        for i in range(0, len(ct_files), batch_size):\n",
    "            batch_files = ct_files[i:i + batch_size]\n",
    "            \n",
    "            ct_batch = []\n",
    "            mask_batch = []\n",
    "            \n",
    "            for f in batch_files:\n",
    "                ct = np.load(os.path.join(ct_path, f))\n",
    "                mask = np.load(os.path.join(mask_path, f))\n",
    "                \n",
    "                ct_batch.append(ct)\n",
    "                mask_batch.append(mask)\n",
    "            \n",
    "            ct_batch = np.array(ct_batch)\n",
    "            mask_batch = np.array(mask_batch)\n",
    "            \n",
    "            # Add channel dimension if needed\n",
    "            ct_batch = np.expand_dims(ct_batch, -1)\n",
    "            mask_batch = np.expand_dims(mask_batch, -1)\n",
    "            \n",
    "            yield ct_batch, mask_batch\n",
    "\n",
    "# Create generators\n",
    "train_generator = create_data_generator(\"processed_data/train\")\n",
    "test_generator = create_data_generator(\"processed_data/test\")\n",
    "\n",
    "print(\"Data generators created and ready for model training!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Create and compile the model\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_unet_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     44\u001b[0m              loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     45\u001b[0m              metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mDice()])\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Define callbacks\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m, in \u001b[0;36mbuild_unet_model\u001b[1;34m(input_shape)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_unet_model\u001b[39m(input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m----> 3\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[43mInput\u001b[49m(input_shape)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Encoder (Contracting Path)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     conv1 \u001b[38;5;241m=\u001b[39m Conv2D(\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m3\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m)(inputs)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Input' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "# Define the U-Net model architecture\n",
    "def build_unet_model(input_shape=(512, 512, 1)):\n",
    "    inputs = Input(input_shape)\n",
    "    \n",
    "    # Encoder (Contracting Path)\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    \n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    \n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    \n",
    "    # Bridge\n",
    "    conv4 = Conv2D(512, 3, activation='relu', padding='same')(pool3)\n",
    "    conv4 = Conv2D(512, 3, activation='relu', padding='same')(conv4)\n",
    "    \n",
    "    # Decoder (Expanding Path)\n",
    "    up5 = concatenate([Conv2DTranspose(256, 2, strides=(2, 2), padding='same')(conv4), conv3], axis=3)\n",
    "    conv5 = Conv2D(256, 3, activation='relu', padding='same')(up5)\n",
    "    conv5 = Conv2D(256, 3, activation='relu', padding='same')(conv5)\n",
    "    \n",
    "    up6 = concatenate([Conv2DTranspose(128, 2, strides=(2, 2), padding='same')(conv5), conv2], axis=3)\n",
    "    conv6 = Conv2D(128, 3, activation='relu', padding='same')(up6)\n",
    "    conv6 = Conv2D(128, 3, activation='relu', padding='same')(conv6)\n",
    "    \n",
    "    up7 = concatenate([Conv2DTranspose(64, 2, strides=(2, 2), padding='same')(conv6), conv1], axis=3)\n",
    "    conv7 = Conv2D(64, 3, activation='relu', padding='same')(up7)\n",
    "    conv7 = Conv2D(64, 3, activation='relu', padding='same')(conv7)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = Conv2D(1, 1, activation='sigmoid')(conv7)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Create and compile the model\n",
    "model = build_unet_model()\n",
    "model.compile(optimizer='adam',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy', tf.keras.metrics.Dice()])\n",
    "\n",
    "# Define callbacks\n",
    "callbacks = [\n",
    "    ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_dice', mode='max'),\n",
    "    EarlyStopping(monitor='val_dice', patience=10, mode='max'),\n",
    "    ReduceLROnPlateau(monitor='val_dice', factor=0.1, patience=5, mode='max')\n",
    "]\n",
    "\n",
    "# Calculate steps per epoch based on dataset size and batch size\n",
    "train_steps = len([f for f in os.listdir(os.path.join(\"processed_data/train\", 'ct_scans')) if f.endswith('.npy')]) // 32\n",
    "val_steps = len([f for f in os.listdir(os.path.join(\"processed_data/test\", 'ct_scans')) if f.endswith('.npy')]) // 32\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_steps,\n",
    "    epochs=100,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=val_steps,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['dice'], label='Training Dice')\n",
    "plt.plot(history.history['val_dice'], label='Validation Dice')\n",
    "plt.title('Model Dice Score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Dice Score')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
